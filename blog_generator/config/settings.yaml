openai:
  model: gpt-4-turbo
  temperature: 0.7
  max_retries: 5
  request_timeout: 45
  rate_limit_per_minute: 60
  max_token_size: 3000

generation:
  parallel_threads: 4
  max_tokens_per_section: 800
  validate_output: false
  validate_readability: false

paths:
  input_csv: input/sample_input.csv
  output_success: output/success/
  output_failure: output/failure/
  output_logs: output/logs/
  output_retries: output/retries/
  section_prompts: config/prompts/section_prompts.json
  master_prompt: config/prompts/master_prompt.md

llm_provider: gemini # openai or gemini

gemini:
  model: gemini-2.0-flash
  temperature: 0.7

llm_section_defaults:
  emotional_hook:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7

  story_part_1:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.8

  story_part_2:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.8

  story_part_3:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.8

  checklist:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.9

  faq_google_autocomplete:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7

  faq_core:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7

  faq_curious:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7

  faq_cta:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7

  cta:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7

  meta_description:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7

  keyword_generation:
    provider: gemini
    model: gemini-1.5-pro
    temperature: 0.7
